
\begin{enumerate}
\item \textbf{Shajratul Alam}, University of Rhode Island \\
Efficient Feature Selection in SVM and GP for Infection Detection in Human Blood Cell Images.

\emph{\footnotesize Shajratul Alam (University of Rhode Island); Tingfang Lee (University of Rhode Island); Shiva Shrestha (University of Rhode Island); Dr. Gavino Puggioni (University of Rhode Island)}

Characterizing patient blood samples is an important step for the diagnosis of blood based diseases. Developing a method that can automatically classify cell images into infected vs non-infected has important implication in the field of medical science. For this study, we chose a dataset consisting of two sets of images - infected with malaria parasite cells and non-infected ones. Our goal is to train a classifier using support vector machine (SVM) and/or gaussian process (GP) that can predict cell type given an image. Usually, such machine learning methods as well as more advanced ones, for example deep neural networks (DNN), use raw cell images as inputs. These approaches have proven to be computationally complex and time demanding. Instead of using raw images, we tried to extract suitable features from the images to train our classifier with a view to reducing number of features and computational time. We found that, for our dataset if the images were to be represented by the descriptive statistics (mean, median, standard deviation, minimum, maximum and quantiles) of RGB channels of the pixel values, the resulting classifier using SVM or GP performs well enough in comparison to those requiring intensive computation.

\item \textbf{Ryan Buckland}, Center for Biomedical Informatics, Brown University \\
A Statistical and Computational Approach for Improving EHR-Based Classification of Suicidal Thoughts and Behaviors

\emph{\footnotesize Ryan S Buckland; Elizabeth S Chen, PhD}

Recent research has demonstrated cohort misclassification (e.g., underreporting) when studies of suicidal thoughts and behaviors (STBs) rely on ICD-9/10-CM diagnosis codes. In response, other data in the Electronic Health Record (EHR) are being explored to better identify patient cohorts, a process called “EHR phenotyping.” These efforts involve incorporating unstructured data (e.g., clinical notes) by applying natural language processing (NLP) to extract, represent, and analyze information captured within narrative text. A scoping review of previous work found many studies applied heuristics and manual processes. Where studies included computational approaches, they often developed custom software, thus posing challenges to reproducibility. Building upon existing applications of NLP for EHR phenotyping in the context of STB, this study uses a publicly-accessible intensive care EHR dataset (Medical Information Mart for Intensive Care III [MIMIC-III]), a well-established NLP program for biomedical text (MetaMap), and regularized logistic regression, an accessible statistical method for classification and feature selection. Taken together, these form a portable and iterable framework that can be used to identify textual features of interest as well as identify misclassified patients in different settings and among different populations.

\item \textbf{Dingxian Cao}, University of Connecticut \\
Asymptotic Inference for a Dynamic Panel Model with Fixed Effects and Time Specific Effects When Unit Roots Exist

\emph{\footnotesize Dingxian Cao(University of Connecticut); Chihwa Kao(University of Connecticut)}

This paper investigates a first-order autoregressive dynamic linear panel data model with both fixed effects and time specific effects, of which the autoregressive parameter is 1. When cross-section dimension (n) and time dimension (T) are large, we derive a limiting distribution of the bias corrected (quasi) maximum likelihood estimator of the autoregressive parameter. We find that the bias correction method proposed by Hahn and Kuersteiner (2002) can also be applied to fixed effects dynamic panel model with time specific effects when the autoregressive parameter is 1.

\item \textbf{Lijiang Geng}, University of Connecticut \\
Subsampled Information Criterion for Bayesian Model Selection in Big Data Setting

\emph{\footnotesize Lijiang Geng (University of Connecticut); Yishu Xue (University of Connecticut); Guanyu Hu (University of Connecticut)}

In big data time, less attention is paid to Bayesian methods as they are known to be computationally intensive for both parameter estimation and model selection, while existing literatures focus more on approaches to speed up Markov chain Monte Carlo (MCMC). Deviance-based model selection criteria, such as the deviance information criterion (DIC) and the Bayesian predictive information criterion (BPIC), are well-known for model selection purposes. In this article, we introduce the subsampled DIC and the subsampled information criterion IC in the big data context. Under reasonable regularity conditions, we show that our proposed subsampled criteria closely approximate their full data counterparts. Extensive simulation studies are conducted to evaluate the empirical performance of the proposed criterion. The usage of our proposed criterion is further illustrated with the analysis of two datasets, a household income data from the 1994 Census, and the Covertype Data Set.

\item \textbf{Gregory Guranich}, University of Massachusetts Amherst \\
Global Estimation of Component Unmet Need for Contraceptives Using a Bayesian Hierarchical Spline Model

\emph{\footnotesize Gregory Guranich, Leontine Alkema}

We propose a Bayesian hierarchical spline model to estimate levels and trends in unmet need for limiting and spacing of childbirth in the world's poorest countries. The model expands the Family Planning Estimation Tool (FPET), used by the United Nations Population Division and FP2020 for estimating and projecting contraceptive use and unmet need, to produce estimates of spacing and limiting. In the model, the unmet need for limiting childbirth is expressed as a proportion of total unmet need for contraceptives. The model utilizes the relation between this proportion and total contraceptive demand, a proxy for how far the contraceptive transition has progressed, to inform the estimates. We validate the model through out-of-sample validation exercises and present estimates and projections from 1990 to 2030.

\item \textbf{Colin Hill}, Yale University \\
Diversity and Inequity in Connecticut Public High Schools: Measuring and Investigating Socioeconomic Disparities Between Schools and Communities

\emph{\footnotesize Colin Hill (Yale University)}

Public schools in America continue to be segregated by race and social class, and there are few efforts at the federal, state and local level to ameliorate these conditions. In fact, efforts like school choice have actually resulted in increased school segregation. In this project, I compile a dataset containing longitudinal information about the racial demographics and educational achievement of traditional public high schools in Connecticut to investigate disparities between the racial demographics of schools and the communities they serve. I begin by evaluating the benefits and drawbacks of three different metrics for measuring school diversity. I then employ clustering methods and linear mixed models to illuminate possible connections between these enrollment disparities and trends in educational achievement. The results indicate that while both schools and their districts are getting more diverse, schools are diversifying much faster. Although wealthier, disproportionately white students attend the least diverse and highest-performing schools, controlling for socioeconomic characteristics of the school and its surrounding district reveals that diversity may have a positive impact on SAT scores but no significant effect on graduation rates. The findings of these analyses have implications for the future of school desegregation initiatives and policies, which must adapt to rapidly shifting student demographics without disproportionately placing the burden of school diversity on marginalized communities.

\item \textbf{Enrique ter Horst}, Universidad de los Andes, Colombia, Bogota \\
Topics and Methods in Economics, Finance, and Business Journals: A Content Analysis Enquiry.

\emph{\footnotesize Jorge Camargo (Konrad Lorenz); Maximiliano Gonzalez (Uniandes); Alexander Guzman (CESA); Enrique Ter Horst (Uniandes); Maria Andrea Trujillo (CESA)}

This study analyzes the abstracts and titles of 33,454 business finance, economics, management, and business articles published in ISI (frequently cited) journals during 2013e14. The sample represents 46.4 percent of all papers published in ISI journals in those years, and 52.7 percent of the articles published in the selected categories. The journals were ranked in four Q categories according to their impact factors. The analysis revealed that some topics persisted in all Q groups, but others gained frequency by Q, which suggests that Q1 journals (those with higher impact factors) create trends that are followed by other publications. All Q groups have a methodological approach that is predominantly empirical rather than theoretical. In addition, while the business and management categories privileged case studies, economics studies emphasized panel data analyses. Finally, our study confirms the relevance of the English language in academia.

\item \textbf{Dongming Huang}, Department of Statistics, Harvard University \\
Relaxing the Assumptions of Knockoffs by Conditioning

\emph{\footnotesize Dongming Huang (Harvard); Lucas Janson (Harvard)}

The recent paper Candes et al. (2018) introduced model-X knockoffs, a method for variable selection that provably and non-asymptotically controls the false discovery rate with no restrictions or assumptions on the dimensionality of the data or the conditional distribution of the response given the covariates. The one requirement for the procedure is that the covariate samples are drawn independently and identically from a precisely-known (but arbitrary) distribution. The present paper shows that the exact same guarantees can be made without knowing the covariate distribution fully, but instead knowing it only up to a parametric model with parameters at the order of np, where p is the dimension and n is the number of covariate samples (which may exceed the usual sample size of labeled samples when unlabeled samples are also available). The key is to treat the covariates as if they are drawn conditionally on their observed value for a sufficient statistic of the model. Although this idea is simple, even in Gaussian models conditioning on a sufficient statistic leads to a distribution supported on a set of zero Lebesgue measure, requiring techniques from topological measure theory to establish valid algorithms. We demonstrate how to do this for three models of interest, with simulations showing the new approach remains powerful under the weaker assumptions.

\item \textbf{Kristen Hunter}, Harvard University \\
Inferring Medication Adherence using Health Outcomes with Bayesian State-Space Models

\emph{\footnotesize Kristen Hunter (Harvard University); Mark Glickman (Harvard University); Luis Campos (Harvard University)}

Patients' non-adherence to their prescribed medication is a serious obstacle to successful medication therapy and a widespread problem in clinical care. Providers and patients are likely more empowered to make more informed decisions if they have accurate information about medication adherence. Current methods to summarize medication adherence are generally not practical or accurate enough to be useful in clinical settings. We develop an approach to infer medication adherence rates from commonly-collected clinical data, including: (1) health outcomes measured over time that are likely to be directly impacted by differential adherence, and (2) baseline health characteristics and sociodemographic data. Our approach uses efficient Bayesian computational methods for the goal of inferring recent adherence behavior, and uses information not typically utilized in adherence models. The method we adopt can be understood in two steps. First, we fit a Bayesian State-Space Model (SSM) to health outcomes as a function of time-varying adherence. Second, we infer a particular patient's medication adherence given their observed health outcomes and baseline health and sociodemographic information using a Sequential Monte Carlo (SMC) algorithm, which accomplishes efficient sampling in high dimensional spaces. Summaries of adherence, including interval estimates, can be determined directly from the SMC posterior draws.

\item \textbf{Khurshid Jahan}, University of Rhode Island \\
Prediction of the Seasonal Changes and Impacts of Road Salt on Suburban Watershed Areas using Artificial Neural Networks (ANN)

\emph{\footnotesize Khurshid Jahan, Soni M. Pradhanang, University of Rhode Island}

Road salts in stormwater runoff in both urban and suburban areas are of concern to many urban and suburban dwellers. The chloride-based de-icers, i.e., sodium chloride (NaCl), magnesium chloride (MgCl2), and calcium chloride (CaCl2) are dissolved in stormwater runoff and also percolated into the soils during storage, transport. Event-based stormwater runoff was considered to predict the chloride concentration and predict the seasonal changes in different sites in a suburban watershed area. Water quality data for 34 rainfall events (2016-2019) greater than 0.5 inch were used in this study. The Artificial Neural Network (ANN) model was developed using rainfall amount, turbidity, total suspended solids (TSS), Dissolved organic carbon (DOC), sodium, chloride, and total nitrate. Data were trained using the Levenberg-Marquardt backpropagation algorithm. The model was applied in six different sites within a small location and predict the seasonal change. Sensitivity analyses was conducted for the study to determine the influence of input variables on the dependent variable to check the uncertainty of the study result. The study aims to develop the model, predict the seasonal changes of the chloride concentration and assess its impact on the roadside areas.

\item \textbf{Peng Jin}, New York University School of Medicine \\
Generalized Mean Residual Life Models for Case-Cohort and Nested Case-Control Studies

\emph{\footnotesize Peng Jin (New York University School of Medicine); Anne Zeleniuch-Jacquotte (New York University School of Medicine); Mengling Liu (New York University School of Medicine);}

Mean residual life (MRL) is defined as the remaining life expectancy of a subject who has survived to a certain time point, and is an alternative way to hazard function for characterizing time-to-event data. Inference and application of MRL models have primarily focused on cohort studies. In practice, case-cohort and nested case-control designs have been commonly used within large cohort studies, particularly when studying costly molecular biomarkers. They enable prospective inference as the full-cohort design with significant cost-saving benefit. In this paper, we study the modeling and inference of a family of generalized MRL models for studies under case-cohort and nested case-control designs. Built upon the idea of inverse selection probability, the weighted estimating equations are constructed to estimate the regression parameters and baseline mean residual life function. Asymptotic properties of the proposed estimators are established and finite-sample performance is evaluated by extensive numerical simulations under various settings. The proposed methodology is demonstrated on the NYU Women's Health Study.

\item \textbf{Dongah Kim}, University of Massachusetts, Amherst \\
Nonparametric Bayesian Population Sizeestimation with Missing Entries

\emph{\footnotesize Dongah Kim, Krista J. Gile}

We proposed a Bayesian nonparametric method for estimating closed population size with missing entries. Based on the Bayesian population size estimation developed by [Manrique-Vallier, 2016], we extended this approach when the missing entries exist. This approach, based on Dirichlet process mixture models, can capture the complex dependencies without specific number of latent classes. To handle the missing entries, we use multiple imputation from the posterior distribution. We apply this new method to simulation study with several scenarios and to real data for deaths in Syria during civil war.

\item \textbf{Joseph A. Langan}, University of Rhode Island \\
A Bayesian State-Space Approach to Improve Projections of Stock Biomass for Managing New England Groundfish

\emph{\footnotesize Joseph A. Langan (University of Rhode Island), Christopher M. Legault (Northeast Fisheries Science Center), Gavino Puggioni (University of Rhode Island), Jason E. McNamee (Rhode Island Division of Marine Fisheries), Jeremy S. Collie (University of Rhode Island)}

Specification of Allowable Biological Catch requires projecting biomass one to three or more years beyond the terminal year of fish stock assessments.  However, these projections are often highly uncertain and can perform poorly under retrospective review.  For many New England groundfish stocks, consistent biases in assessments, known as retrospective errors, have led to overestimation of biomass resulting in unintentional overfishing, sharp reductions in catch quotas, and decreased stakeholder confidence in the management process. In an effort to address such issues, this work will develop a Bayesian state-space model aimed at improving projections of fish stock abundance. Our approach will allow for the inclusion of climate data, climate data, expert input (e.g. use of estimates of biological parameters from the literature as prior information), and inference from similar species, where appropriate, to make full use of all data available to inform stock projections. The performance of the developed modeling framework, compared with existing approaches, is evaluated both in simulation and through retrospective forecasting of assessment data for three data-rich and three data-limited New England groundfish stocks by calculating the prediction-error variance of the difference between the realized and projected stock biomass.

\item \textbf{Joochul Lee}, University of Connecticut \\
Online Updating Method to Correct for Measurement Error in Big Data Streams

\emph{\footnotesize Elizabeth Schifano (University of Connecticut), Haiying Wang (University of Connecticut)}

When huge amounts of data arrive in streams, online updating is an important method to alleviate both computational and data storage issues. This paper extends the scope of previous research for online updating in the context of the classical linear measurement error model. In the case where some covariates are unknowingly measured with error at the beginning of the stream, but then are measured without error after a particular point along the data stream, the updated estimators ignoring the measurement error are biased for the true parameters. We propose a method to correct the bias of the estimators, as well as correct their variances, once the covariates measured without error are first observed; after correction, the traditional online updating method can then proceed as usual. We further derive the asymptotic distributions for the corrected and updated estimators. We provide simulation studies and a real data analysis with the Airline on-time data to illustrate the performance of our proposed method.

\item \textbf{Myeonggyun Lee}, New York University School of Medicine \\
Empirical Comparison of Sub-Cohort Sampling Designs for Breast Cancer Risk Prediction Model on the NYU Woman’s Health Study (NYUWHS)

\emph{\footnotesize Myeonggyun Lee (NYU School of Medicine); Anne Zelenuich-Jacquotte (NYU School of Medicine); Mengling Liu (NYU School of Medicine)}

As molecular biomarkers are increasingly considered in risk modeling for complex diseases (e.g. cancer), within-cohort sampling designs, such as nested case-control and case cohort studies, have been widely used to estimate an individual’s risk. For estimating the effects of risk factors, within-cohort sampling designs have been shown to maintain relatively high efficiency with respect to the prospective full-cohort design while having advantages of saving costs in data assembly. The performance of building risk prediction models under sub-cohort sampling designs, however, is under-studied comparing to the full cohort analysis. Moreover, when conducting sub-cohort sampling, we may consider a further matching procedure, such as gender or exact subject’s age, to obtain accurate estimates of relative risks. The estimation and prediction ability of sub-cohort sampling designs could heavily depend on the matching procedure. In this paper, we performed extensive numerical studies to compare risk prediction models using nested case-control and case cohort designs with the full cohort study. A primary aim of our simulation is to evaluate how sensitive the risk prediction measures are to the underlying sub-cohort sampling designs with or without matching confounders. We used a NYU Woman’s Health Study (NYUWHS) dataset as a platform for our numerical studies.

\item \textbf{Tingfang Lee}, University of Rhode Island \\
Skellam Process Modeling for Financial High-Frequency Data

\emph{\footnotesize TingFang Lee (University of Rhode Island); Gavino Puggioni (University of Rhode Island)}

High-frequency data are observations collected at fine time scale. Such data largely incorporates pricing and transactions, of which institutional rules prevent from drastically rising or falling within a short period of time. This results in data changes based on the measure of one tick, a measure of the minimum upward or downward movement in the price of a security.  The discreteness brings that the observations are in $\mathbb{Z}$. A Skellam distribution has a unique property that returns values in $\mathbb{Z}$. 

We are interested in studying the Skellam process where the time-dependent intensities are Gaussian process. Such doubly stochastic Poisson process, also known as Cox process, is a point process which is a generalization of a Poisson process. We then investigate if this Skellam models provide better fit to high frequency financial data and how Gaussian process can capture the market microstructure.

\item \textbf{Moming Li}, George Mason University \\
On Symmetric Semiparametric Two-Sample Problem

\emph{\footnotesize Moming Li (George Mason University); Guoqing Diao (George Mason University); Jing Qin (National Institute of Allergy and Infectious Diseases)}

We consider a two-sample problem where data come from symmetric distributions. Usual two-sample data with only magnitudes recorded, arising from case-control studies or logistic discriminant analyses, may constitute a symmetric two-sample problem. We propose a semiparametric model such that, in addition to symmetry, the log ratio of two unknown density functions is modeled in a known parametric form. The new semiparametric model, tailor-made for symmetric two-sample data, can also be viewed as a biased sampling model subject to symmetric constraint. A maximum empirical likelihood estimation approach is adopted to estimate the unknown model parameters, and the corresponding profile empirical likelihood ratio test is utilized to perform hypothesis testing regarding the two population distributions. Symmetry, however, comes with irregularity. It is shown that, under the null hypothesis of equal symmetric distributions, the maximum empirical likelihood estimator has degenerate Fisher information, and the test statistic has a mixture of Chi-squared asymptotic distribution. Extensive simulation studies have been conducted to demonstrate promising statistical powers under correct and mis-specified models. We apply the proposed methods to two real examples.

\item \textbf{Yan Li}, University of Connecticut \\
Weight Matrix Construction in Fingerprinting to Improve Detection of Global Temperature Signals in Historical Climate

\emph{\footnotesize Yan Li (University of Connecticut); Kun Chen (University of Connecticut); Jun Yan (University of Connecticut); Xuebin Zhang (Environment and Climate Change Canada)}

The classic fingerprinting method for detection and attribution analysis of climate change is a multiple regression where the regression coefficients are the target of the inferences. The conclusions of detection and attribution analyses depend on the confidence intervals for the regression coefficients of the signals of the external forcings. In practice, such analyses are complicated by the fact that both the signals and weight matrix need to be estimated. Under these complications, the performance of the optimal fingerprinting greatly depends on the accuracy in weight matrix estimation. In this paper, We propose a strategy of obtaining the weight matrix based on the idea that shorter confidence intervals of the resulting regression coefficient estimators can improve the efficiency of detection and attribution analysis. We directly construct weight matrix from loss function targeting on the variance of the resulting regression coefficient estimators. Our theoretical results show that the proposed weight matrix is more efficient than the existing linear shrinkage weight asymptotically as the sample size and the dimension go to infinity together, in the context of both generalized least squares (GLS) and generalized total least squares (GTLS). Our extensive simulation study shows that in finite sample, the proposed approach is at least competitive to existing method and has better performance in some realistic situations with respect to the length of confidence intervals and empirical coverage rates.

\item \textbf{Elise Lim}, Boston University \\
A Unified Method for Rare Variant Analysis of Gene-Environment Interactions

\emph{\footnotesize Elise Lim (Boston University); Han Chen (the University of Texas Health Science Center at Houston); Jos\\\'ee Dupuis (Boston University); Ching-Ti Liu (Boston University)}

Advanced technology in whole-genome sequencing has offered the opportunity to comprehensively investigate the genetic contribution, particularly rare variants, to complex traits. Several region-based tests have been developed to jointly model the marginal effect but methods to detect gene-environment (GE) interactions are underdeveloped. Identifying the modification effects of environmental factors on genetic risk poses a considerable challenge. To tackle this challenge, we develop a method to detect GE interactions of a set of rare variants using generalized linear mixed model. The proposed method can accommodate either binary or continuous traits in related or unrelated samples. Under this model, genetic main effects, GE interactions, and sample relatedness are modeled as random effects. We adopt a kernel-based method to leverage the joint information across rare variants and implement variance component score tests to reduce the computational burden. Our simulation studies of continuous and binary traits show that the proposed method maintains correct type I error rates and high power under various scenarios, such as genotype main effects and GE interactions in opposite directions and the proportion of causal variants in the model for both continuous and binary traits. We apply our method in the Framingham Heart Study to test GE interaction of smoking on body mass index or overweight status and replicate the CHRNB4 gene association reported in previous large consortium meta-analysis of single nucleotide polymorphism (SNP)-smoking interaction. Our proposed set-based GE test is computationally efficient and is applicable to both binary and continuous phenotypes, while appropriately accounting for familial or cryptic relatedness.

\item \textbf{Wodan Ling}, Columbia University \\
Responder Analysis by Deep Learning with Application to a Phase III Clinical Trial

\emph{\footnotesize Wodan Ling (Columbia University), Qi Tang (Sanofi), Youran Qi (University of Wisconsin-Madison)}

Responder analysis is routinely conducted in clinical trials to examine the treatment effect of an investigational product or medical practice. Its response is the probability that a patient is a responder, i.e. the chance that one has a response exceeding a predefined threshold. This analysis provides clinically meaningful results and may complement findings on the original continuous scale of the endpoint. To estimate the probabilities, the continuous response variable is often discretized into a categorical variable. Generalized linear models are then used to model the relationship between covariates and the discretized response. However, it may suffer from the violation of the linearity assumption. To overcome such a limitation, we propose a deep responder analysis method, which takes a deep learning approach without making a linearity assumption and offers automatic feature representation embedded. However, discretization of a continuous response variable was adopted in this method, which may lead to loss of power. Thus, in an attempt to improve the method, a second method is proposed, which avoids discretization while still enabling inference on probabilities of being a responder. The second method may be superior to the first one if there is no measurement error in the response variable and an appropriate loss function is chosen. We illustrate the two methods using a clinical trial example in the area of asthma.

\item \textbf{Xiaokang Liu}, University of Connecticut \\
Multivariate Functional Regression via Nested Reduced-Rank Regularization

\emph{\footnotesize Xiaokang Liu (University of Connecticut); Shujie Ma (University of California, Riverside); Kun Chen (University of Connecticut)}

To achieve sufficient dimensionality reduction while allowing for model flexibility and interpretability, we propose a nested reduced-rank regression (NRRR) approach in multivariate functional linear models with multivariate functional responses and predictors. Specifically, our model is based on a two-layer low-rank structure imposed on the functional regression surfaces. A global low-rank structure captures the potential grouping among the individual functional responses and/or the functional predictors, aiming to recover a smaller set of latent “principal” functional responses and predictors that drive the underlying functional association. A local low-rank structure, on the other hand, offers control on the complexity of the association between the latent principal functional responses and predictors. For estimation, we adopt a pragmatic basis expansion approach that induces smoothness in the regression surface and converts an infinite dimensional problem to a finite dimensional one. Model estimation is conducted by minimizing a mean integrated squared error criterion subject to the nested low-rank constraints, for which an iterative algorithm is developed with a convergence guarantee. Theoretically, we show that NRRR can at least achieve a comparable error rate to the reduced rank estimator. Simulation studies demonstrate the effectiveness of NRRR and the power of global dimension reduction in the presence of highly correlated functional variables. We apply NRRR to an electricity demand dataset to relate the trajectories of the daily electricity consumption with those of the daily temperatures.

\item \textbf{Fang Liu}, Northeast Normal University and University of Connecticut \\
A Generalized Semi-Parametric Model for Jointly Analyzing Response Times and Accuracy in Computerized Testing

\emph{\footnotesize FANG LIU(Northeast Normal University and University of Connecticut); Ming-Hui Chen(University of Connecticut); Jiwei Zhang(Yunnan University); Ningzhong Shi(Northeast Normal University)}

The Cox proportional hazards model (1972, 1975) has been widely used for modeling response time data in educational and psychological research. However, based on the Kaplan-Meier (KM) plots in an empirical example, we find that the proportionality of the hazard ratios does not seem to be an appropriate assumption. There are considerable differences in survival rates among different items.  To overcome such a problem, we consider a class of nonproportional hazards latent trait models known as

generalized odds-rate class of regression models. This class is general enough to include several commonly used models, including proportional hazards latent trait model and proportional odds latent trait model, as special cases. The new model associating with the item response model is embedded within the hierarchical framework proposed by van der Linden to characterize the relationship between the response times and response accuracy by a population model. A fully Bayesian method is adopted for parameter estimation and the deviance information criterion (DIC) and the logarithm of the pseudomarginal likelihood (LPML) are employed for model comparison. Three simulation studies are conducted and a detailed analysis of the Programme for International Student Assessment (PISA) science data is carried out to further illustrate the proposed methodology.

\item \textbf{Fernanda Maciel}, Bentley University \\
The Impact of Bolsa Família on Educational Indicators in Brazil

\emph{\footnotesize Fernanda Maciel (Bentley University)}

The objective of this paper is to assess the impact of Brazil's Bolsa Família conditional cash-transfer program on schooling outcomes of children between 6 and 15 years old. Using the method of Instrumental Variables (IV) to control for endogeneity, I show that by participating in the program, students decrease the chance of dropping out of school by 1.6\%. As a comparison, the probability of dropping out among students outside of the range of compliance is 12.3\%. Another finding is that the probability of repeating a grade in school increases by 9.2\%, which might be caused by the family keeping the children in school just to comply with the requirements of this program.

\item \textbf{Thomas McAndrew}, University of Massachusetts at Amherst \\
Adaptively Stacked Ensembles for Influenza Forecasting with Incomplete Data

\emph{\footnotesize Thomas McAndrew (University of Massachusetts at Amherst); Nicholas G. Reich (University of Massachusetts at Amherst)}

Seasonal Influenza infects an average 30 million people in the United States every year, overburdening hospitals during weeks of peak incidence. Named by the CDC as an important tool to fight the damaging effects of these epidemics, accurate forecasts of influenza and influenza like illness (ILI) forewarn public health officials about when, and where, seasonal influenza outbreaks will hit hardest.

Ensemble forecasts have shown positive results in forecasting 1 to 4 week ahead ILI percentages better than any single model in an ensemble. But current ensemble forecasts are static within a season, training on past ILI data before the season begins and generating optimal weights for each model kept constant throughout the season. We propose a novel adaptive ensemble forecast capable of changing model weights week-by-week throughout the flu season, only needing current flu season data to make predictions, and by adding a prior, able to moderate abrupt changes in ensemble weights that may be caused by mid-seasons revisions to past ILI data.

During the course of the flu season hospitals and agencies experience reporting delays, changing ILI percentages from past weeks. ILI percentages only considered final when the flu season ends, adding a prior distribution over ensemble weights allows us to account for incomplete, weekly-changing historical data. At the start of the season and without any data, models are weighted equally. After observing how the ensemble's models perform on week ahead forecasts, a variational inference algorithm updates model weights and forecasts the next 4 weeks of ILI percentages.

Obtaining an optimal prior weighting equal to 8\% of the training data to account for incomplete ILI percentages, our adaptive ensemble shows comparable performance to the static ensemble season to season (mean \% dif. $log-score = 0.03$; $95CI = [-0.01,0.08]$).

In settings without substantial historical training data (i.e. emerging pandemics) or when new forecasting models do not have a long track-record of performance, an adaptive ensemble approach is a valuable option for performance-based weighting of models, enhancing the public health impact of ensemble forecasts.

\item \textbf{Paul Mclaughlin}, University of Connecticut \\
A Spatial Capture-Recapture Model with Dependent Animal Movement

\emph{\footnotesize Paul McLaughlin (UConn); Haim Bar (Uconn)}

Over the past two decades there have been many advancements in modeling capture-recapture (CR) data to account for emerging data collection technology and techniques. Spatial capture-recapture (SCR) models have been introduced to estimate population size and numerous other demographic parameters from spatially explicit CR data and more recently have incorporated realistic animal movement schemes. While some species of animals are known to exhibit attractive behavior, nearly all SCR models assume complete independence amongst individual’s movement and capture probability. In this paper we introduce a SCR model which allows for attractions between individuals via their daily movements. We demonstrate accounting for this dependence can improve the population size estimation via a simulation study. Additionally, we apply our model to an iconic SCR dataset to estimate the population size and attraction parameters of a Bengal tiger (Panthera tigris tigris) population.

\item \textbf{Jinjian Mu}, University of Connecticut \\
Bayesian Variable Selection for Cox Regression Model with Spatially Varying Coefficients with Applications to Louisiana Respiratory Cancer Data

\emph{\footnotesize Jinjian Mu (University of Connecticut); Qingyang Liu (University of Connecticut); Lynn Kuo (University of Connecticut); Guanyu Hu (University of Connecticut)}

The Cox regression model is a commonly used model in survival analysis. In public health studies, clinical data are often collected from medical service providers of different locations. There are large geographical variations in the survival rates from cancer. In this paper, we focus on the variable selection issue for Cox regression model with spatially varying coefficients. We propose a Bayesian hierarchical model for variable selection where horseshoe prior and point mass mixture prior are employed for sparsity and determining whether a covariate is spatially varying. An efficient two-stage computational method is used for posterior inference and variable selection. It essentially applies the existing method for maximizing partial likelihood for the Cox model by site independently first, and then we develop an MCMC algorithm for variable selection based on the results of the first stage. Extensive simulation studies based on this method are carried out to examine the empirical performance of the proposed method. Finally, we apply the proposed methodology to analyzing a real data set on respiratory cancer in Louisiana from the SEER program.

\item \textbf{Meirose Neal}, Stonehill College \\
An Item Response Theory Based Growth Measure for the Rhode Island Comprehensive Assessment System

\emph{\footnotesize Eugene Quinn (Stonehill College), Nicholas Ramirez (Stonehill College), Kendra Adams (Stonehill College), and MeiRose Neal (Stonehill College)}

In 2018 the Rhode Island Department of Education implemented a new standardized test, the Rhode Island Comprehensive Assessment System.  They did not change the measure of student ``growth'', which is the Colorado model or Student Growth Percentile system (SGP). SGP was developed nearly 20 years ago and is not specific to the RICAS. We examine the feasibility of using an Item Response Theory (IRT) model to construct a measure student ''growth'' that is tailored to the RICAS.

\item \textbf{Isabel Nowinowski}, University of Rhode Island \\
A Network Perspective on Student Collaboration in an Introductory Statistics Course

\emph{\footnotesize Isabel Nowinowski (University of Rhode Island), Dr. Natallia Katenka (University of Rhode Island)}

Generation Z (individuals born between the mid-1990s and early 2010s), characterized as tech-savvy, independent, and visual, is beginning to graduate college and enter the workforce. While significant research effort has focused on understanding the learning preferences of the preceding Millennial generation (individuals born between the early 1980s and mid-1990s), less is known about how technology has shaped the educational expectations and learning preferences of Generation Z. Gaining insight into how this generation learns would allow universities to modify and enhance course structures and teaching methodologies to better suit this incoming generation of students. The goal of this study is to distinguish the learning habits of Generation Z by understanding the main drivers of student collaborations in an undergraduate Biostatistics course at the University of Rhode Island. To learn about Generation Z, we use survey data collected in Spring 2017 where students were asked to disclose study habits, pre and post-course attitudes, stress levels, learning tasks, teaching techniques, and any collaborators they partnered with throughout the semester. In this poster, we present a visual and descriptive analysis of the network of student collaborators enrolled in an introductory Biostatistics course. The association between the number of collaborators and student course performance are quantified using correlation analysis. Preliminary results using Exponential Random Graph Models (ERGM) explore the effect of student characteristics including demographics, attitudes towards Statistics, and study preferences on the formation of student collaborations. Outcomes from this study may provide educators with insight into the relationships between student characteristics, course performance, and collaborations. Educators may be empowered to modify course structures to facilitate student collaboration and enhance learning outcomes and course performance.

\item \textbf{Yulei Pang}, SCSU \\
Will You Graduate on Time? Ask Big Data!

\emph{\footnotesize Yulei Pang (SCSU)}

Low graduation rate is a significant and growing problem in U.S. higher education systems. Although previous studies have demonstrated the usefulness of building statistical models for predicting students’ graduation outcomes, advanced machine learning models promise to improve the effectiveness of these models, and hone in on the “difference that makes a difference” not only on the group level, but also on the level of the individual student. In this paper we propose an ensemble support vector machines based model for predicting students’ graduation. Up to about 100 features, including a set of psychological-educational factors, were employed to construct the predicting model. We evaluated the proposed model using data taken from a state university’s longitudinal, cohort data sets from the incoming classes of students from 2011-2012 (n=350). The experimental results demonstrated the effectiveness of the model, with considerable accuracy, precision, and recall. This paper presents the results of analysis that were conducted in order to gauge the predictive capability of a machine learning algorithm to predict on-time graduation that took into consideration students’ learning and development.

\item \textbf{Qi Qi}, University of Connecticut \\
Predicting Incident Alzheimer's Disease Using a New Classification System Based on Objective Memory Impairment Assessment

\emph{\footnotesize Qi Qi (University of Connecticut); Lynn Kuo (University of Connecticut); Susan Resnick (National Institute on Aging); Ellen Grober (Albert Einstein College of Medicine)}

Stages of Objective Memory Impairment (SOMI) has been recently proposed by Grober \textit{et al}. (2018) as a new classification system that provides a clinical vocabulary for describing the type and severity of episodic memory impairment in preclinical Alzheimer's disease (AD). We evaluate the diagnostic accuracy of SOMI using a joint model for the time to AD and SOMI that is assessed longitudinally. In particular, we estimate the sensitivity and specificity of SOMI at 3, 5, or 7 years from the baseline assessment for each subject using all subsequent assessments. Our method was applied to the Baltimore Longitudinal Study of Aging. The receiver operating characteristic curve and the corresponding area under it show that SOMI has potential for predicting incident Alzheimer disease. Years of education significantly improved prediction compared to SOMI alone.

\item \textbf{John J Ragland II}, University of Rhode Island \\
The Elegance of the Adult Hermaphrodite C. Elegans Connectome

\emph{\footnotesize John Ragland (University of Rhode Island); Natallia Katenka (University of Rhode Island)}

The C. elegans nematode has been the subject of intense interest in computational biology. The goal of the present research is to derive insights into the biological neural network of the adult hermaphrodite of this elegant worm species. To this end, we performed network analysis on a canonical connectome compromising 302 neurons, four general areas of the worm, and 2345 connections from neurons. This connectome can be represented as a weighted, directed graph, where each edge weight corresponds to the approximate strength of signaling from a neuron to another node. Our findings include the following: the network is degree disassortative; it exhibits the preferential attachment and small-world properties; its edge weight distribution can be approximated by a shifted negative binomial; it can be partitioned into a set of 14 densely connected meta-nodes (each corresponding to a detected community) plus 9 isolated nodes; the connectome notably differs from a random graph; and preliminary exponential random graph modeling shows some ability for edge weights to be predicted by other network characteristics.

\item \textbf{Shiva Gopal Shrestha}, URI \\
Performance of Support Vector Machine  and Gaussian Processes Amidst Rapidly Evolving  Machine Learning Algorithms in Imbalanced Datasets

\emph{\footnotesize Shiva Gopal Shrestha (University of Rhode Island); Shajratul Alam (University of Rhode Island); Tingfang Lee (University of Rhode Island); Gavino Puggioni (University of Rhode Island)}

Machine learning (ML) algorithms for various classification problems are growing rapidly. However, Support Vector Machines (SVM)  and Gaussian Processes (GP) are still famous for their strengths and performance. The popularity of SVM  is large in  ML owing to its underlying solid mathematical background, high generalization capability in finding  global minimums and non-linear classification solutions. However, when exposed to imbalanced datasets, the skewness towards minority class degrades the performance and yields suboptimal results. This shortcoming can be dealt by using various resampling approaches. GP define distributions on functions which can be used in classification problems as well. Compared to SVMs, the strength of GP lies in integrated feature selection, fully probabilistic predictions and interpretability. Moreover, overfitting, the biggest challenge in ML today, does not apply to fully Bayesian methods. Also, when coupled with resampling approaches for imbalanced datasets, GP are very competitive with SVM. The advent of new  boosting algorithms like Random Forest, XGBoost , AdaBoost Classifier , LightGBM etc are considered are highly efficient classifiers. In this project , we have used a highly imbalanced dataset, the Credit Card Fraud dataset. The results  show that the performance of SVM and GP are comparable to aforementioned algorithms. The choice of the best method is always data dependent and will be determined under metrics such as model complexity,computational complexity, explainability, and ease of implementation.

\item \textbf{Haiyan Su}, Montclair State University \\
Generalized p-Values for Testing Zero-Variance Components in Linear Mixed-Effects Models

\emph{\footnotesize Haiyan Su (Montclair State University), Xinmin Li (Qingdao University), Hua Liang (George Washington University)}

Linear mixed-effects models are widely used in analysis of longitudinal data. However, testing for zero-variance components of random effects has not been well resolved in statistical literature, although some likelihood-based procedures have been proposed and studied. In this work, we propose a generalized p-value based method to tackle this problem.  The proposed method is also applied to test linearity of the nonparametric functions in additive models. We provide theoretical justifications and develop an implementation algorithm for the proposed method. We evaluate its finite-sample performance and compare it with that of the restricted likelihood ratio test via simulation experiments. The proposed approach is illustrated by using an application from a nutritional study.

\item \textbf{Herbert Susmann}, University of Massachusetts Amherst \\
Subnational Estimation of Modern Contraceptive Use in Kenya

\emph{\footnotesize Herb Susmann (University of Massachusetts at Amherst), Krzysztof Sakrejda (University of Massachusetts Amherst), Chuchu Wei (University of Massachusetts at Amherst), Leontine Alkema (University of Massachusetts Amherst)}

Subnational estimates of contraceptive use are important for countries to inform their family planning policies. Currently, national-level estimates of contraceptive use are generated by the United Nations Population Division and the FP2020 initiative using the Family Planning Estimation Tool (FPET), a multilevel Bayesian model that incorporates data from diverse data sources. We extend the FPET model to produce estimates within countries. Spatial modeling is incorporated when appropriate to capture spatial correlation in the rate and timing of the growth of modern contraceptive use among counties. The model flexibly incorporates subnational prevalence data when available, and otherwise falls back to national-level data. We illustrate our approach by modeling modern contraceptive use by county in Kenya.

\item \textbf{Jianing Wang}, Boston Medical Center, Boston University School of Public Health \\
Selection from Multiple Data Sources in a Capture-Recapture Study to Estimate the Prevalence of Opioid Use Disorder in Massachusetts

\emph{\footnotesize Jianing Wang, MSc (Boston Medical Center); Joshua A. Barocas, MD (Boston Medical Center, Boston University School of Medicine); Dana Bernson, MPH (Massachusetts Department of Public Health); Benjamin P. Linas, MD, MPH (Boston Medical Center, Boston University School of Medicine); Laura F. White, PhD (Boston University)}

Background and Objects: Current prevalence estimates of opioid use disorder (OUD) are derived from direct measurement or estimation. One such estimation method is known as capture-recapture. However, there is little guidance regarding the appropriate number of databases and the dependence structures between them that is needed for accurate estimates using this method. We propose a process to create a parsimonious model from multiple data sources in a capture-recapture study to estimate the prevalence of OUD in Massachusetts.

Methods: We previously published an analysis of a series of equally well-fitting capture-recapture models in the negative binomial modeling framework to estimate the size of the unknown population of OUD in Massachusetts from 2011-2015. We used the Massachusetts Public Health Data Warehouse (PHD, previously known as the “Chapter 55 dataset”) for the analysis. The PHD links records at the person-level across 22 unique datasets. We used seven datasets to identify the OUD population including: the All-Payer Claims Database (APCD), Bureau of Substance Addiction Services (BSAS), Hospital, ED, and Outpatient Observation discharges (Case Mix), Death certificates (Death), Birth certificates (Birth), Emergency Medical Service Incident Data (MATRIS) (available between 2013-2015), and Prescription Monitoring Program (PMP). Persons from six administrative databases for 2011-2012 and seven for 2013-2015 were included in the analysis. We used data from 2015 to create the process for our parsimonious model. Using the seven available databases, we assessed the difference from the baseline analysis in the results from the models with fewer data sources and more constraints on the underlying dependence structure. Specifically, we constructed a series of models covering all possible permutations of three databases (35 combinations), four databases (35 combinations), five databases (21 combinations), and six databases (7 combinations). We analyzed these models by incorporating two-way interaction terms only and then three-way interaction terms additionally. We compared the extrapolation from different scenarios to the estimates from the baseline analysis to determine parsimonious combinations of data sources and the corresponding dependence structure. We compared the parsimonious combinations to investigate the balance between number of databases and complexity of the dependence structures. 

Results: We have found that the majority (>90\%) of the “known” population is accounted for by using the APCD, BSAS, Case Mix, and PMP databases. In general, in the combinations of more than four databases, there were strong positive two-way interactions between these four databases. In the model that included only these four databases, the incorporation of three-way interaction terms provided estimates covered by the confidence interval of the estimates in the baseline study. However, the model that included these four databases but only two-way interaction terms provided an inadequate fit to the data. Other databases have overlap with these four datasets in varying degrees. For example, the Death dataset significantly negatively interacted with APCD, but had a strong positive relationship with the Case Mix database.

Conclusion: This process suggested that in capture-recapture analyses, a greater number of data sources provide more accurate estimates of the unknown population of opioid use disorder than fewer data sources. With more than five data sources available, the discrepancy of the estimation between the models with up to three-way interaction terms and the models only with two-way interaction terms is less significant. However, the incorporation of three-way interaction terms in the models with fewer data sources makes significant over-estimation much more likely. Therefore, in the cases of having fewer data sources, incorporating up to two-way interaction terms is suggested.

\item \textbf{Zhe Wang}, University of Connecticut \\
Sequential Fixed-Width Confidence Interval Estimation and Minimum Risk Point Estimation for the Mean of a Normal Population: Sampling in Groups

\emph{\footnotesize Nitis Mukhopadhyay (University of Connecticut); Zhe Wang (University of Connecticut)}

In this paper, we revisit the two fundamental problems on sequential estimation: the fixed-width confidence interval estimation problem and the minimum risk point estimation problem, both in the context of estimating an unknown mean in a Normal population where the population variance is assumed unknown. In purely sequential procedure, we sample one observation at a time. In many situations, it is more convenient and economical to sample a group of observations at a time. This paper discusses the possibility of extending the purely sequential procedure to a more general purely group sequential procedure. Instead of taking one observation at a time, we propose sampling k more observations sequentially, and replacing the sample standard deviation with more general estimators in defining the stopping rule. Asymptotic first- and second-order efficiency and asymptotic first- and second-order consistency properties have been discussed. Also, the requirement of initial sample size is given to achieve the consistency and efficiency. Simulation studies are included. A breast cancer data is used as an illustration. The simulation result and real data analysis show the overall goodness of purely group sequential procedure.

\item \textbf{Wenshuo Wang}, Harvard University \\
Metropolized Knockoff Sampling

\emph{\footnotesize Stephen Bates (Stanford); Emmanuel Cand\u00e8s (Stanford); Lucas Janson (Harvard); Wenshuo Wang (Harvard)}

Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeability property with the explanatory variables under study. This paper introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis–Hastings formulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model.

\item \textbf{Shuang Wang}, University of Rhode Island \\
Bayesian Latent Class Model for Predicting Gestational Age Using an Administrative Database

\emph{\footnotesize Shuang Wang, Khurshid Jahan, Manushi K. v. Welandawe, Jacob Strock, Gavino Puggioni, Xuerong Wen (University of Rhode Island)}

Administrative Databases play an important role in research on drug safety in pregnancy. Its usefulness may be limited by lacking an accurate estimate of gestational age at birth (GAB). As an important index for pre-/neonatal healthcare and the onset of pregnancy, GAB is typically assessed by ultrasound and reported in the Birth Certificate. In the absence of clinically reported GAB, we focus on developing a Bayesian latent class model to predict GAB. Our Bayesian approach allows modeling heterogeneity in the population by identifying latent subgroups and estimation of the individual characteristics that affect the latent class memberships. The hierarchical model that we propose requires two sets of covariates for class membership and to predict GAB within each class. Estimation is conducted using Markov Chain Monte Carlo (MCMC). We use DIC and predictive measures as criteria for selecting an optimal number of classes. With Medicaid data linked with Birth Certificate from the Department of Health, Rhode Island, the model is first estimated using 60\% of the observations and then validated over the remaining 40\%. Altogether, our model shows to have greater flexibility compared to other approaches, accuracy in estimation, and prediction performance.

\item \textbf{Zhengfan Wang}, UMASS-Amherst \\
Estimating Stillbirth Rates for all Countries from 2000 Till 2017 using a Bayesian Temporal Hierarchical Regression Model

\emph{\footnotesize Zhengfan Wang; Leontine Alkema}

Estimation of stillbirths globally is complicated because of the absence of reliable data from countries where most stillbirth occur. We compile stillbirth rate (SBR) data from vital registration systems, censuses and surveys, and obtained information on candidate predictors for 195 countries from 2000 until 2017. We propose a Bayesian temporal hierarchical regression model for SBR estimation for all countries from 2000 to 2017. The model accounts for differences in bias and error variances across different data source types, includes country-level intercepts that are estimated hierarchically, and a temporal smoothing process to capture data-driven fluctuations in country-periods with high quality data. Horseshoe priors are used for variable selection. We validate model performance through in-sample and out-of-sample validation. 


\item \textbf{Chuchu Wei}, University of Massachusetts Amherst \\
Estimating Pregnancies, Pregnancy Intentions, and Abortions for all Countries from 1990 to 2020 using a Bayesian Hierarchical Latent Variable Model

\emph{\footnotesize Chuchu Wei, Jonathan Bearak, Leontine Alkema}

Estimates of pregnancies, pregnancy intentions, and abortions can help assess how effectively women and couples are able to fulfill their childbearing aspirations. However, estimating these variables for all countries is challenging because of substantial data missingness: pregnancy rates are generally not observed, and countries may lack data on abortion rates and/or the proportion of births that are intended, for some or all time periods in question. For these reasons, a standard regression-based approach to estimation may produce questionable results. To address these issues, we propose a theoretically grounded Bayesian model which jointly estimates pregnancies and associated outcomes of interest as functions of the numbers of women by country-period subgroups, defined by marital status, contraceptive need and use. The model is parametrized using a set of latent parameters, which are country-period- subgroup specific pregnancy rates and propensities to abort. Hierarchical temporal smoothing models are used to allow for estimation in data-poor settings. In this poster, we present the proposed model and the results from a simulation exercise that assesses identifiability of latent model parameters based on global data availability.

\item \textbf{Kristina Yamkovoy}, Boston University \\
Estimating the Reproductive Number for Tuberculosis in the United States

\emph{\footnotesize Kristina Yamkovoy (Boston University, Department of Biostatistics), Laura F. White (Boston University, Department of Biostatistics)}

Tuberculosis (TB) is the leading cause of death due to infectious disease globally, yet little is understood about its transmission patterns. The reproductive number, which is the average number of cases that an infected individual creates, is useful for monitoring the effect of interventions and understanding transmission dynamics at a population level. This quantity, while studied extensively in other infectious diseases, is not well-understood for TB. Of particular interest is understanding how transmission dynamics vary among populations by factors such as age and location, leading to more insightful estimates of the reproductive number and indicating which groups are contributing toward ongoing transmission, and thus, where to focus interventions. We show how methods that have been developed for other diseases using routinely collected surveillance data can be modified for estimation of the reproductive number of TB. We calculate region-specific reproductive numbers for TB in the United States between 2012-2016 while accounting for the effect of immigration. The estimated reproductive number for TB hovers around one for the overall US, yet decreases to 0.8 as immigration is incorporated. The reproductive number tended to be highest in the South and lowest in the Midwest and Northeast. Statistical tools, such as to estimate the reproductive number, are valuable to monitoring and shedding insight on transmission dynamics for TB.

\item \textbf{Guandong Yang}, University of Massachusetts Amherst \\
Quantifying the Contribution of Population-Period-Specific Information to Model-Based Estimates in Demography and Global Health: Technical Considerations

\emph{\footnotesize Guandong Yang (UMass Amherst);  Leontine Alkema (UMass Amherst); Krista Gile (UMass Amherst)}

Sophisticated statistical models are used to produce estimates for demographic and health indicators even when data are limited, very uncertain or lacking. We aim to provide a standardized approach to calculate a data weight $w$ that quantifies the extent to which a model-based estimate of an indicator of interest $\mu$ is informed by data $y$ for the relevant population-period as opposed to information supplied by other periods and populations and model assumptions. In addition, we aim to propose a data-model accordance measure $a$ which quantifies how extreme the population-period data are relative to the prior model-based prediction. This paper provides candidate expressions for $w$ and $a$ and assesses their performance in various settings when the only information available constitutes of a limited number of samples from the model-based prior $p(\mu)$ and posterior distribution $p(\mu|y)$.

\item \textbf{Ali Yousefi}, Boston University \\
Assessing Goodness of Fit in Marked-Point Process Models

\emph{\footnotesize Ali Yousefi, Uri T. Eden}

Marked-point process modeling framework has received increasing recent attention for real-time analysis of neural spike trains. For the marked-point process modeling framework, an important aspect is the development of goodness-of-fit techniques which enable us to have an accurate and interpretable assessment of the fitted model to the observed full spike events. In this talk, we present a set transformation, which takes the observed spike events and generate new data points that are uniformly and identically distributed in the new mark and time spaces. These transformations are scalable to multi-dimensional mark spaces and provide independent and identically distributed (i.i.d) samples in hypercubes, which are best suited for uniformity tests. We discuss properties of these transformations and demonstrate what aspect(s) of a model fit is captured per each transformation. We also discuss a list of uniformity tests to assess whether the transformed events are distributed according to multivariate uniform distribution or not. We demonstrate applications of these transformation and uniformity tests in both simulation data and neural data samples.

\item \textbf{Hang Yu}, University of North Carolina at Chapel Hill \\
Nonparametric and Computationally Efficient Feature Selection with Regularized Tensor Product Kernel

\emph{\footnotesize Hang Yu (University of North Carolina at Chapel Hill ); Yuanjia Wang (Columbia University); Donglin Zeng (University of North Carolina at Chapel Hill)}

With growing interest to use black-box machine learning for complex data with many feature variables, it is critical to obtain a prediction model that only depends on a small set of features to maximize  generalizability.

Therefore,  feature selection remains to be an important and challenging problem in modern applications. Most of existing methods for feature selection are based on either parametric or semiparametric models, so the resulting performance can severely suffer from model misspecification when high-order nonlinear interactions among the features are present.  A very limited number of approaches for nonparametric feature selection were proposed, but they are  computationally intensive and may not even converge. In this paper, we propose a novel and computationally efficient approach for nonparametric feature selection based on a tensor-product kernel function over the feature space. The importance of each feature is governed by a parameter in the kernel function which can be efficiently computed iteratively from a modified alternating direction method of multipliers (ADMM) algorithm. We prove the oracle selection property of the proposed method. Finally, we demonstrate the superior performance of our approach compared to existing methods via  simulation studies and applications to the prediction of Alzheimer's disease.

\item \textbf{Jiahui Yu}, University of Massachusetts Amherst \\
Smoothing Spline Semiparametric Density Models

\emph{\footnotesize Jiahui Yu* (UMass Amherst); Jian Shi* (UCSB); Anna Liu (UMass Amherst); Yuedong Wang (UCSB) (*joint First Authors)}

Density estimation plays a fundamental role in many areas of statistics and machine learning. Semiparametric density models are flexible in incorporating domain knowledge and uncertainty regarding the shape of the density function. We consider a unified framework based on reproducing kernel Hilbert space for modeling, estimation, computation and theory. Our proposed general semiparametric density models include many existing models as special cases. We develop penalized likelihood based estimation methods and computational methods under different situations. We also establish joint consistency and derive convergence rates of the proposed estimators, as well as the convergence rate of the overall density function. Lastly, we validate our estimation methods empirically through simulations and an application.

\item \textbf{Zhonghui Zhang}, University of Connecticut \\
Mahalanobis Metric Based Clustering for Fixed Effects Model

\emph{\footnotesize Zhonghui Zhang, Chihwa Kao, Min Seong Kim}

This paper improves the estimation procedure in the linear panel data model with time-varying group fixed effects in Bonhomme and Manresa (2015). We introduce a Mahalanobis distance-based K-means algorithm (KMM) which takes serial correlation and heteroscedasticity into a count. It will significantly improve the accuracy of estimation in the case of ellipse-shaped data. The Euclidean distance failed in such a situation. Also, we find that the estimator of the common parameter may not converge to the true value during the iteration procedure in Bonhomme and Manresa (2015) since the true value is not the best choice concerning clustering. We compute the infeasible optimal option of beta given the specific structure. Lastly, we provide our empirical method to amplify the group signal and hence improve the estimation of group membership and then the estimator of the common parameter.

\item \textbf{Isaac Zhao}, Brown University \\
Developing Nonlinear Genetic Signatures for Enzalutamide Resistance in Prostate Cancer

\emph{\footnotesize Isaac Zhao (Brown University), Kathryn E. Ware (Duke University Medical Center), Jason A. Somarelli (Duke University Medical Center), Andrew J. Armstrong (Duke University Medical Center), and Lorin Crawford (Brown University)}

Background: Early-stage prostate cancer requires normal androgen levels for growth (with testosterone
being the most abundant) and can be treated with androgen dependent therapy (ADT).
However, malignancies inevitably advance to the castration resistant stage (CRPC) in which cancerous
growth continues despite very low levels of circulating testosterone. Enzalutamide, a second-generation
androgen receptor (AR) inhibitor medication, is an effective first line therapeutic treatment for
CRPC. However, due to surviving tumor cells, most patients relapse after treatment and develop a
resistance to enzalutamide as well. Further understanding of these resistance mechanisms are needed
to enhance targeted treatment effectiveness for prostate cancer.

Objective: This project specifically aims to identify the genetic signature components and associated
pathways with the largest functioning role for enzalutamide resistance in prostate cancer.

Methods: Statistical analysis was split into two stages. For the first stage, differences in genetic
signatures between pre-treatment and acute treatment response were analyzed using a gaussian
process classification model. A recently developed nonlinear effect size analog and a state-of-the-art
variable importance measure (RATE) are then used to interpret genetic signature significance which
incorporates both marginal effects, as well as nonlinear interactions. For the second stage, genetic
expression differences between acute treatment response and acquired chronic resistance are compared via a conventional t-test to identify genomic drivers. Notable genes of importance are then mapped to cellular hallmark pathways to pinpoint key biological mechanisms necessary for prostate cancer survival in the presence enzalutamide.

Results: Between on-treatment and relapse, we identified many differentially expressed pathways and
gene sets. These results are consistent with other published findings and include G2M checkpoints,
androgen response related mechanisms, and E2F targets.

Conclusions: Comparing our findings with other studies highlights promising candidates as resistance
mechanisms to enzalutamide. Lesser documented genes and pathways may be novel elements that
are identified because of the incorporation of nonlinear genetic covarying relationships. These may
be worth further investigation.

\item \textbf{Xiaojing Zhu}, Boston University \\
Random Graph Hidden Markov Models for Percolation in Noisy Dynamic Networks

\emph{\footnotesize Eric Kolaczyk (Boston University); Heather Shappell (Johns Hopkins University)}

In the study of random networks, percolation – the sudden emergence of a giant connected component (GCC) – is of fundamental interest. Traditionally, work has concentrated on noise-free percolation with a monotonic process of network growth, but real-world networks are more complex. We develop a class of random graph hidden Markov models (RG-HMMs) for characterizing percolation regimes in noisy, dynamically evolving networks in the presence of both edge birth and edge death. This class subsumes a variety of random graph models already used in studying noise-free percolation. We focus on parameter estimation and testing of putative percolation regimes.  We present an Expectation-Maximization (EM) algorithm, incorporating data augmentation and particle filtering, for estimating parameters in the model with a given sequence of noisy networks observed only at a longitudinal subsampling of time points. This in turn facilitates development of hypothesis testing strategies aimed ultimately at inferring putative percolation mechanisms in epileptic seizures.

\end{enumerate}

